{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from langsmith.wrappers import wrap_openai\n",
    "from langsmith import traceable\n",
    "import os\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "import time\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain import hub\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langsmith.evaluation import evaluate\n",
    "\n",
    "# Grade prompt\n",
    "grade_prompt_answer_accuracy = prompt = hub.pull(\"langchain-ai/rag-answer-vs-reference\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"]=os.getenv(\"openai_key\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"]=\"true\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"]=\"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"]=os.getenv(\"langsmith_api_key\")\n",
    "pinecone_api_key=os.environ.get(\"PINECONE_API_KEY\")\n",
    "index_name=\"test-cs-240711\"  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "LangSmithConflictError",
     "evalue": "Conflict for /datasets. HTTPError('409 Client Error: Conflict for url: https://api.smith.langchain.com/datasets', '{\"detail\":\"Dataset with this name already exists.\"}')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\XEVEN\\miniconda3\\envs\\langsmith\\Lib\\site-packages\\langsmith\\utils.py:121\u001b[0m, in \u001b[0;36mraise_for_status_with_text\u001b[1;34m(response)\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 121\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mHTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\XEVEN\\miniconda3\\envs\\langsmith\\Lib\\site-packages\\requests\\models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[1;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mHTTPError\u001b[0m: 409 Client Error: Conflict for url: https://api.smith.langchain.com/datasets",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\XEVEN\\miniconda3\\envs\\langsmith\\Lib\\site-packages\\langsmith\\client.py:799\u001b[0m, in \u001b[0;36mClient.request_with_retries\u001b[1;34m(self, method, pathname, request_kwargs, stop_after_attempt, retry_on, to_ignore, handle_response, **kwargs)\u001b[0m\n\u001b[0;32m    789\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession\u001b[38;5;241m.\u001b[39mrequest(\n\u001b[0;32m    790\u001b[0m         method,\n\u001b[0;32m    791\u001b[0m         (\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    797\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mrequest_kwargs,\n\u001b[0;32m    798\u001b[0m     )\n\u001b[1;32m--> 799\u001b[0m \u001b[43mls_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status_with_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    800\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mc:\\Users\\XEVEN\\miniconda3\\envs\\langsmith\\Lib\\site-packages\\langsmith\\utils.py:123\u001b[0m, in \u001b[0;36mraise_for_status_with_text\u001b[1;34m(response)\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mHTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 123\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mHTTPError(\u001b[38;5;28mstr\u001b[39m(e), response\u001b[38;5;241m.\u001b[39mtext) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mHTTPError\u001b[0m: [Errno 409 Client Error: Conflict for url: https://api.smith.langchain.com/datasets] {\"detail\":\"Dataset with this name already exists.\"}",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mLangSmithConflictError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Define dataset: these are your test cases\u001b[39;00m\n\u001b[0;32m      7\u001b[0m dataset_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest-cs-240711-updated\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 8\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mA sample dataset for hr system in dialog 360\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m client\u001b[38;5;241m.\u001b[39mcreate_examples(\n\u001b[0;32m     10\u001b[0m     inputs\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m     11\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_question\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat specific topics does the Invoicing Support team offer support for?\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     34\u001b[0m     dataset_id\u001b[38;5;241m=\u001b[39mdataset\u001b[38;5;241m.\u001b[39mid,\n\u001b[0;32m     35\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\XEVEN\\miniconda3\\envs\\langsmith\\Lib\\site-packages\\langsmith\\client.py:2422\u001b[0m, in \u001b[0;36mClient.create_dataset\u001b[1;34m(self, dataset_name, description, data_type)\u001b[0m\n\u001b[0;32m   2401\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Create a dataset in the LangSmith API.\u001b[39;00m\n\u001b[0;32m   2402\u001b[0m \n\u001b[0;32m   2403\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2415\u001b[0m \u001b[38;5;124;03m    The created dataset.\u001b[39;00m\n\u001b[0;32m   2416\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2417\u001b[0m dataset \u001b[38;5;241m=\u001b[39m ls_schemas\u001b[38;5;241m.\u001b[39mDatasetCreate(\n\u001b[0;32m   2418\u001b[0m     name\u001b[38;5;241m=\u001b[39mdataset_name,\n\u001b[0;32m   2419\u001b[0m     description\u001b[38;5;241m=\u001b[39mdescription,\n\u001b[0;32m   2420\u001b[0m     data_type\u001b[38;5;241m=\u001b[39mdata_type,\n\u001b[0;32m   2421\u001b[0m )\n\u001b[1;32m-> 2422\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest_with_retries\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2423\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPOST\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2424\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/datasets\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2425\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mContent-Type\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mapplication/json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2426\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2427\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2428\u001b[0m ls_utils\u001b[38;5;241m.\u001b[39mraise_for_status_with_text(response)\n\u001b[0;32m   2429\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ls_schemas\u001b[38;5;241m.\u001b[39mDataset(\n\u001b[0;32m   2430\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse\u001b[38;5;241m.\u001b[39mjson(),\n\u001b[0;32m   2431\u001b[0m     _host_url\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_host_url,\n\u001b[0;32m   2432\u001b[0m     _tenant_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_optional_tenant_id(),\n\u001b[0;32m   2433\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\XEVEN\\miniconda3\\envs\\langsmith\\Lib\\site-packages\\langsmith\\client.py:835\u001b[0m, in \u001b[0;36mClient.request_with_retries\u001b[1;34m(self, method, pathname, request_kwargs, stop_after_attempt, retry_on, to_ignore, handle_response, **kwargs)\u001b[0m\n\u001b[0;32m    831\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ls_utils\u001b[38;5;241m.\u001b[39mLangSmithNotFoundError(\n\u001b[0;32m    832\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResource not found for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpathname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    833\u001b[0m     )\n\u001b[0;32m    834\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m409\u001b[39m:\n\u001b[1;32m--> 835\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ls_utils\u001b[38;5;241m.\u001b[39mLangSmithConflictError(\n\u001b[0;32m    836\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConflict for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpathname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    837\u001b[0m     )\n\u001b[0;32m    838\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    839\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ls_utils\u001b[38;5;241m.\u001b[39mLangSmithError(\n\u001b[0;32m    840\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmethod\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpathname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in LangSmith\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    841\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m API. \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    842\u001b[0m     )\n",
      "\u001b[1;31mLangSmithConflictError\u001b[0m: Conflict for /datasets. HTTPError('409 Client Error: Conflict for url: https://api.smith.langchain.com/datasets', '{\"detail\":\"Dataset with this name already exists.\"}')"
     ]
    }
   ],
   "source": [
    "from langsmith import Client\n",
    "from langsmith.evaluation import evaluate\n",
    "\n",
    "client = Client()\n",
    "\n",
    "# Define dataset: these are your test cases\n",
    "dataset_name = \"test-cs-240711-updated\"\n",
    "dataset = client.create_dataset(dataset_name, description=\"A sample dataset for hr system in dialog 360\")\n",
    "client.create_examples(\n",
    "    inputs=[\n",
    "        {\"input_question\": \"What specific topics does the Invoicing Support team offer support for?\"},\n",
    "        {\"input_question\": \"What should a CS agent do before escalating an issue to Invoicing Support if messaging is impacted?\"},\n",
    "        {\"input_question\": \"What is the purpose of the Synchronize WABA feature in the WABA section?\"},\n",
    "        {\"input_question\": \"What should you do if a client encounters a 402 error when sending a message in WABA?\"},\n",
    "        {\"input_question\": \"What is the role of a Business Solutions Provider (BSP) in accessing the WhatsApp API?\"},\n",
    "        {\"input_question\": \"What does On-Premise refer to in the context of WhatsApp API setup?\"},\n",
    "        {\"input_question\": \"What does the abbreviation WABA stand for, and what does it represent?\"},\n",
    "        {\"input_question\": \"What is the first step in migrating from an On-Premise setup to a 360Dialog hosted solution?\"},\n",
    "        {\"input_question\": \"Why might disabling two-factor authentication be helpful during the migration process?\"},\n",
    "        {\"input_question\": \"What does 360Dialog do after setting up the new WhatsApp Business API client?\"},\n",
    "    ],\n",
    "    outputs=[\n",
    "        {\"output_answer\": \"The Invoicing Support team provides assistance with direct billing invoice disputes, partner paid invoice disputes, refunds over 50 EUR/USD for subscription fees, increasing billing for Multiconnect setups, and payment enforcement topics.\"},\n",
    "        {\"output_answer\": \"A CS agent should first resolve any messaging issues before escalating to Invoicing Support, except if a number is disabled due to non-payment.\"},\n",
    "        {\"output_answer\": \"The Synchronize WABA feature is used to pull the latest data for WABA.\"},\n",
    "        {\"output_answer\": \" If a client encounters a 402 error, the solution is to set the payment method for WABA.\"},\n",
    "        {\"output_answer\": \"A Business Solutions Provider (BSP) like 360dialog allows access to the WhatsApp API, with about 80 such providers available globally.\"},\n",
    "        {\"output_answer\": \"On-Premise refers to a setup where the technology stack for the WhatsApp API is hosted either by 360dialog on their infrastructure or by the client on their own infrastructure.\"},\n",
    "        {\"output_answer\": \"WABA stands for WhatsApp Business Account, representing the account used for business communication on WhatsApp.\"},\n",
    "        {\"output_answer\": \"The first step is to back up the settings data from the current WhatsApp Business API client\"},\n",
    "        {\"output_answer\": \"Disabling two-factor authentication is helpful if the authentication code is forgotten and re-registration is needed, although re-registration is not required for a smooth migration.\"},\n",
    "        {\"output_answer\": \"After setting up the new client, 360Dialog logs into the new instance, performs a restore of the settings, and conducts a health check to ensure everything is working properly.\"},\n",
    "    ],\n",
    "    dataset_id=dataset.id,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test-cs-240711-updated'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the cost and timeline from API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://api.smith.langchain.com/public/b37ca9b1-60cd-4a2a-817e-3c4e4443fdc0/run?exclude_s3_stored_attributes=false\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"test-cs-240711\"\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'34bd534b-0900-43bd-b9a3-846b3b8fd823'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pinecone_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc = Pinecone(api_key=pinecone_api_key)\n",
    "vectorstore = PineconeVectorStore(index_name=index_name, embedding=embeddings)\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RAG bot\n",
    "\n",
    "import openai\n",
    "from langsmith import traceable\n",
    "from langsmith.wrappers import wrap_openai\n",
    "\n",
    "class RagBot:\n",
    "\n",
    "    def __init__(self, retriever, model: str = \"gpt-4-0125-preview\"):\n",
    "        self._retriever = retriever\n",
    "        # Wrapping the client instruments the LLM\n",
    "        self._client = wrap_openai(openai.Client())\n",
    "        self._model = model\n",
    "\n",
    "    @traceable()\n",
    "    def retrieve_docs(self, question):\n",
    "        return self._retriever.invoke(question)\n",
    "\n",
    "    @traceable()\n",
    "    def invoke_llm(self, question, docs):\n",
    "        response = self._client.chat.completions.create(\n",
    "            model=self._model,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"You are a helpful AI code assistant with expertise in cs related queries.\"\n",
    "                    \" Use the following docs to produce a concise code solution to the user question.\\n\\n\"\n",
    "                    f\"## Docs\\n\\n{docs}\",\n",
    "                },\n",
    "                {\"role\": \"user\", \"content\": question},\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        # Evaluators will expect \"answer\" and \"contexts\"\n",
    "        return {\n",
    "            \"answer\": response.choices[0].message.content,\n",
    "            \"contexts\": [str(doc) for doc in docs],\n",
    "        }\n",
    "\n",
    "    @traceable()\n",
    "    def get_answer(self, question: str):\n",
    "        docs = self.retrieve_docs(question)\n",
    "        return self.invoke_llm(question, docs)\n",
    "\n",
    "rag_bot = RagBot(retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Invoicing Support team offers support for the following specific topics:\\n\\n1. Direct Billing Invoice disputes (overcharging, undercharging, client '"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = rag_bot.get_answer(\"What specific topics does the Invoicing Support team offer support for?\")\n",
    "response[\"answer\"][:150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_rag_answer(example: dict):\n",
    "    \"\"\"Use this for answer evaluation\"\"\"\n",
    "    response = rag_bot.get_answer(example[\"input_question\"])\n",
    "    return {\"answer\": response[\"answer\"]}\n",
    "\n",
    "def predict_rag_answer_with_context(example: dict):\n",
    "    \"\"\"Use this for evaluation of retrieved documents and hallucinations\"\"\"\n",
    "    response = rag_bot.get_answer(example[\"input_question\"])\n",
    "    return {\"answer\": response[\"answer\"], \"contexts\": response[\"contexts\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Response vs reference answer (Answer/response correctness)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a teacher grading a quiz. \n",
      "\n",
      "You will be given a QUESTION, the GROUND TRUTH (correct) ANSWER, and the STUDENT ANSWER. \n",
      "\n",
      "Here is the grade criteria to follow:\n",
      "(1) Grade the student answers based ONLY on their factual accuracy relative to the ground truth answer. \n",
      "(2) Ensure that the student answer does not contain any conflicting statements.\n",
      "(3) It is OK if the student answer contains more information than the ground truth answer, as long as it is factually accurate relative to the  ground truth answer.\n",
      "\n",
      "Score:\n",
      "A score of 1 means that the student's answer meets all of the criteria. This is the highest (best) score. \n",
      "A score of 0 means that the student's answer does not meet all of the criteria. This is the lowest possible score you can give.\n",
      "\n",
      "Explain your reasoning in a step-by-step manner to ensure your reasoning and conclusion are correct. \n",
      "\n",
      "Avoid simply stating the correct answer at the outset.\n"
     ]
    }
   ],
   "source": [
    "print(grade_prompt_answer_accuracy[0].prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def answer_evaluator(run, example) -> dict:\n",
    "    \"\"\"\n",
    "    A simple evaluator for RAG answer accuracy\n",
    "    \"\"\"\n",
    "\n",
    "    # Get question, ground truth answer, RAG chain answer\n",
    "    input_question = example.inputs[\"input_question\"]\n",
    "    reference = example.outputs[\"output_answer\"]\n",
    "    prediction = run.outputs[\"answer\"]\n",
    "\n",
    "    # LLM grader\n",
    "    llm = ChatOpenAI(model=\"gpt-4-turbo\", temperature=0)\n",
    "\n",
    "    # Structured prompt\n",
    "    answer_grader = grade_prompt_answer_accuracy | llm\n",
    "\n",
    "    # Run evaluator\n",
    "    score = answer_grader.invoke({\"question\": input_question,\n",
    "                                  \"correct_answer\": reference,\n",
    "                                  \"student_answer\": prediction})\n",
    "    score = score[\"Score\"]\n",
    "\n",
    "    return {\"key\": \"answer_v_reference_score\", \"score\": score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_results = evaluate(\n",
    "    predict_rag_answer,\n",
    "    data=dataset_name,\n",
    "    evaluators=[answer_evaluator],\n",
    "    experiment_prefix=\"cs-rag-answer-v-reference\",\n",
    "    metadata={\"version\": \"cs context, gpt-4-0125-preview\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Response vs input (response relevance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "grade_prompt_answer_helpfulness = prompt = hub.pull(\"langchain-ai/rag-answer-helpfulness\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a teacher grading a quiz. \n",
      "\n",
      "You will be given a QUESTION and a STUDENT ANSWER. \n",
      "\n",
      "Here is the grade criteria to follow:\n",
      "(1) Ensure the STUDENT ANSWER is concise and relevant to the QUESTION\n",
      "(2) Ensure the STUDENT ANSWER helps to answer the QUESTION\n",
      "\n",
      "Score:\n",
      "A score of 1 means that the student's answer meets all of the criteria. This is the highest (best) score. \n",
      "A score of 0 means that the student's answer does not meet all of the criteria. This is the lowest possible score you can give.\n",
      "\n",
      "Explain your reasoning in a step-by-step manner to ensure your reasoning and conclusion are correct. \n",
      "\n",
      "Avoid simply stating the correct answer at the outset.\n"
     ]
    }
   ],
   "source": [
    "print(grade_prompt_answer_helpfulness[0].prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grade prompt\n",
    "\n",
    "def answer_helpfulness_evaluator(run, example) -> dict:\n",
    "    \"\"\"\n",
    "    A simple evaluator for RAG answer helpfulness\n",
    "    \"\"\"\n",
    "\n",
    "    # Get question, ground truth answer, RAG chain answer\n",
    "    input_question = example.inputs[\"input_question\"]\n",
    "    prediction = run.outputs[\"answer\"]\n",
    "\n",
    "    # LLM grader\n",
    "    llm = ChatOpenAI(model=\"gpt-4-turbo\", temperature=0)\n",
    "\n",
    "    # Structured prompt\n",
    "    answer_grader = grade_prompt_answer_helpfulness | llm\n",
    "\n",
    "    # Run evaluator\n",
    "    score = answer_grader.invoke({\"question\": input_question,\n",
    "                                  \"student_answer\": prediction})\n",
    "    score = score[\"Score\"]\n",
    "\n",
    "    return {\"key\": \"answer_helpfulness_score\", \"score\": score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test-cs-240711-updated'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'cs-rag-answer-helpfulness-ec989b02' at:\n",
      "https://smith.langchain.com/o/9bdcd0b2-8b47-501f-b66b-d379cce39a32/datasets/b30a204e-c905-40d8-a9a9-530289e86bcf/compare?selectedSessions=d0194078-619f-4a4c-b003-b0e2c4207e41\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:27,  2.75s/it]\n"
     ]
    }
   ],
   "source": [
    "experiment_results = evaluate(\n",
    "    predict_rag_answer,\n",
    "    data=dataset_name,\n",
    "    evaluators=[answer_helpfulness_evaluator],\n",
    "    experiment_prefix=\"cs-rag-answer-helpfulness\",\n",
    "    metadata={\"version\": \"cs context, gpt-4-0125-preview\"},\n",
    "    blocking=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'results': [EvaluationResult(key='answer_helpfulness_score', score=1, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('458fc167-4dcf-4f6d-b5fe-7ff38390fd6e'), target_run_id=None)]}\n",
      "{'results': [EvaluationResult(key='answer_helpfulness_score', score=1, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('83b18c3c-6436-46d2-be8d-955cd9cdfbc6'), target_run_id=None)]}\n",
      "{'results': [EvaluationResult(key='answer_helpfulness_score', score=1, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('ba76ea1a-31a6-4276-9742-5fa584851cf1'), target_run_id=None)]}\n",
      "{'results': [EvaluationResult(key='answer_helpfulness_score', score=1, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('53b1fde4-d39a-40e5-b36d-44605d67c4cd'), target_run_id=None)]}\n",
      "{'results': [EvaluationResult(key='answer_helpfulness_score', score=1, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('42f2d825-d04d-4519-8bf9-63d7ce8ac613'), target_run_id=None)]}\n",
      "{'results': [EvaluationResult(key='answer_helpfulness_score', score=1, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('a1c5ff4e-85f4-4909-979a-61e59896b301'), target_run_id=None)]}\n",
      "{'results': [EvaluationResult(key='answer_helpfulness_score', score=1, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('f01dcd58-f84d-454f-8df4-43f5cc6aa751'), target_run_id=None)]}\n",
      "{'results': [EvaluationResult(key='answer_helpfulness_score', score=1, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('91e9b09f-c9be-4a93-8375-86558df4a579'), target_run_id=None)]}\n",
      "{'results': [EvaluationResult(key='answer_helpfulness_score', score=1, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('090893a8-c5f7-4755-97e9-6fd730fb3a71'), target_run_id=None)]}\n",
      "{'results': [EvaluationResult(key='answer_helpfulness_score', score=1, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('121bfd4b-9956-4ea4-981c-2b4ce74250a5'), target_run_id=None)]}\n"
     ]
    }
   ],
   "source": [
    "experiment_results\n",
    "for i, result in enumerate(experiment_results):\n",
    "    print(result['evaluation_results'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Response vs retrieved docs (Groundidness/faithfulness)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a teacher grading a quiz. \n",
      "\n",
      "You will be given FACTS  and a STUDENT ANSWER. \n",
      "\n",
      "Here is the grade criteria to follow:\n",
      "(1) Ensure the STUDENT ANSWER is grounded in the FACTS. \n",
      "(2) Ensure the STUDENT ANSWER does not contain \"hallucinated\" information outside the scope of the FACTS.\n",
      "\n",
      "Score:\n",
      "A score of 1 means that the student's answer meets all of the criteria. This is the highest (best) score. \n",
      "A score of 0 means that the student's answer does not meet all of the criteria. This is the lowest possible score you can give.\n",
      "\n",
      "Explain your reasoning in a step-by-step manner to ensure your reasoning and conclusion are correct. \n",
      "\n",
      "Avoid simply stating the correct answer at the outset.\n"
     ]
    }
   ],
   "source": [
    "grade_prompt_hallucinations = prompt = hub.pull(\"langchain-ai/rag-answer-hallucination\")\n",
    "print(grade_prompt_hallucinations[0].prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def answer_hallucination_evaluator(run, example) -> dict:\n",
    "    \"\"\"\n",
    "    A simple evaluator for generation hallucination\n",
    "    \"\"\"\n",
    "\n",
    "    # RAG inputs\n",
    "    input_question = example.inputs[\"input_question\"]\n",
    "    contexts = run.outputs[\"contexts\"]\n",
    "\n",
    "    # RAG answer\n",
    "    prediction = run.outputs[\"answer\"]\n",
    "\n",
    "    # LLM grader\n",
    "    llm = ChatOpenAI(model=\"gpt-4-turbo\", temperature=0)\n",
    "\n",
    "    # Structured prompt\n",
    "    answer_grader = grade_prompt_hallucinations | llm\n",
    "\n",
    "    # Get score\n",
    "    score = answer_grader.invoke({\"documents\": contexts,\n",
    "                                  \"student_answer\": prediction})\n",
    "    score = score[\"Score\"]\n",
    "\n",
    "    return {\"key\": \"answer_hallucination\", \"score\": score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'rag-answer-hallucination-f2d2cbc3' at:\n",
      "https://smith.langchain.com/o/9bdcd0b2-8b47-501f-b66b-d379cce39a32/datasets/b30a204e-c905-40d8-a9a9-530289e86bcf/compare?selectedSessions=5682d77d-d656-4ff5-b293-4e5e89d0ec52\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:29,  2.99s/it]\n"
     ]
    }
   ],
   "source": [
    "experiment_results = evaluate(\n",
    "    predict_rag_answer_with_context,\n",
    "    data=dataset_name,\n",
    "    evaluators=[answer_hallucination_evaluator],\n",
    "    experiment_prefix=\"rag-answer-hallucination\",\n",
    "    metadata={\"version\": \"cv context, gpt-4-0125-preview\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'results': [EvaluationResult(key='answer_hallucination', score=1, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('20a4c8ac-cba4-4f4e-ad04-8383f84384ec'), target_run_id=None)]}\n",
      "{'results': [EvaluationResult(key='answer_hallucination', score=1, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('ac25beab-138e-49bd-b79b-42be30ced29d'), target_run_id=None)]}\n",
      "{'results': [EvaluationResult(key='answer_hallucination', score=1, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('c3389fe2-b1f6-4b6f-acfc-27f64053d677'), target_run_id=None)]}\n",
      "{'results': [EvaluationResult(key='answer_hallucination', score=1, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('392d0a2d-a4bb-4a2f-8cf3-50e3381858ce'), target_run_id=None)]}\n",
      "{'results': [EvaluationResult(key='answer_hallucination', score=1, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('9bd9a17c-08af-4617-a63f-c4d042911906'), target_run_id=None)]}\n",
      "{'results': [EvaluationResult(key='answer_hallucination', score=1, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('1f347a28-d3a6-469d-9abe-f004408d7058'), target_run_id=None)]}\n",
      "{'results': [EvaluationResult(key='answer_hallucination', score=1, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('a87f4c65-194a-4432-a88e-93919d3aa23d'), target_run_id=None)]}\n",
      "{'results': [EvaluationResult(key='answer_hallucination', score=1, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('8b961b17-dc65-472a-ba18-ec4fe23d68c6'), target_run_id=None)]}\n",
      "{'results': [EvaluationResult(key='answer_hallucination', score=1, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('081ab817-f7cd-4372-a221-24538b93cd2b'), target_run_id=None)]}\n",
      "{'results': [EvaluationResult(key='answer_hallucination', score=1, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('ce39d1cb-c379-44b3-b993-310d2e8f7c4d'), target_run_id=None)]}\n"
     ]
    }
   ],
   "source": [
    "for i, result in enumerate(experiment_results):\n",
    "    print(result['evaluation_results'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieved docs vs input (context relevance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a teacher grading a quiz. \n",
      "\n",
      "You will be given a QUESTION and a set of FACTS provided by the student. \n",
      "\n",
      "Here is the grade criteria to follow:\n",
      "(1) You goal is to identify FACTS that are completely unrelated to the QUESTION\n",
      "(2) If the facts contain ANY keywords or semantic meaning related to the question, consider them relevant\n",
      "(3) It is OK if the facts have SOME information that is unrelated to the question (2) is met \n",
      "\n",
      "Score:\n",
      "A score of 1 means that the FACT contain ANY keywords or semantic meaning related to the QUESTION and are therefore relevant. This is the highest (best) score. \n",
      "A score of 0 means that the FACTS are completely unrelated to the QUESTION. This is the lowest possible score you can give.\n",
      "\n",
      "Explain your reasoning in a step-by-step manner to ensure your reasoning and conclusion are correct. \n",
      "\n",
      "Avoid simply stating the correct answer at the outset.\n"
     ]
    }
   ],
   "source": [
    "grade_prompt_doc_relevance = hub.pull(\"langchain-ai/rag-document-relevance\")\n",
    "print(grade_prompt_doc_relevance[0].prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def docs_relevance_evaluator(run, example) -> dict:\n",
    "    \"\"\"\n",
    "    A simple evaluator for document relevance\n",
    "    \"\"\"\n",
    "\n",
    "    # RAG inputs\n",
    "    input_question = example.inputs[\"input_question\"]\n",
    "    contexts = run.outputs[\"contexts\"]\n",
    "\n",
    "    # LLM grader\n",
    "    llm = ChatOpenAI(model=\"gpt-4-turbo\", temperature=0)\n",
    "\n",
    "    # Structured prompt\n",
    "    answer_grader = grade_prompt_doc_relevance | llm\n",
    "\n",
    "    # Get score\n",
    "    score = answer_grader.invoke({\"question\":input_question,\n",
    "                                  \"documents\":contexts})\n",
    "    score = score[\"Score\"]\n",
    "\n",
    "    return {\"key\": \"document_relevance\", \"score\": score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'rag-doc-relevance-3fba8dfe' at:\n",
      "https://smith.langchain.com/o/9bdcd0b2-8b47-501f-b66b-d379cce39a32/datasets/b30a204e-c905-40d8-a9a9-530289e86bcf/compare?selectedSessions=25222e7b-d46a-43dc-9d21-75badfbfae7f\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:35,  3.52s/it]\n"
     ]
    }
   ],
   "source": [
    "experiment_results = evaluate(\n",
    "    predict_rag_answer_with_context,\n",
    "    data=dataset_name,\n",
    "    evaluators=[docs_relevance_evaluator],\n",
    "    experiment_prefix=\"rag-doc-relevance\",\n",
    "    metadata={\"version\": \"cv context, gpt-4-0125-preview\"},\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langsmith",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
